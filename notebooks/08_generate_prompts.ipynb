{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ae7389c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d145c63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_challenges_path = os.path.join(\"../data/arc-agi-2025/sample_augmented/arc-agi_training_challenges.json\")\n",
    "training_solutions_path = os.path.join(\"../data/arc-agi-2025/sample_augmented/arc-agi_training_solutions.json\")\n",
    "validation_challenges_path = os.path.join(\"../data/arc-agi-2025/sample_augmented/arc-agi_evaluation_challenges.json\")\n",
    "validation_solutions_path = os.path.join(\"../data/arc-agi-2025/sample_augmented/arc-agi_evaluation_solutions.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c24c8a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ARC-AGI Prompt and Output Templates\n",
    "# ============================================\n",
    "\n",
    "# Prompt template\n",
    "ARC_PROMPT_TEMPLATE = \"\"\"You are given 10 example input-output grid pairs from the ARC (Abstraction and Reasoning Corpus) task.\n",
    "Each grid is represented as a 2D array of integers ranging from 0 to 9. Each integer corresponds to a specific color:\n",
    "\n",
    "0 - Black  \n",
    "1 - Blue  \n",
    "2 - Red  \n",
    "3 - Green  \n",
    "4 - Yellow  \n",
    "5 - Gray  \n",
    "6 - Magenta  \n",
    "7 - Orange  \n",
    "8 - Light Blue  \n",
    "9 - Dark Red  \n",
    "\n",
    "Each cell in the grid represents one colored square. The transformation from input to output follows a specific visual or logical pattern.\n",
    "\n",
    "Your task:\n",
    "1. Study the given 10 example input-output pairs carefully.\n",
    "2. Some examples may be incorrect or noisy — identify the pattern that the *majority* of examples follow.\n",
    "3. Infer the correct transformation rule that maps the input grid to the output grid.\n",
    "4. Apply this inferred transformation to the provided test input grid to produce the correct output grid.\n",
    "\n",
    "Below are the example pairs (in JSON format):\n",
    "\n",
    "{examples}\n",
    "\n",
    "Now, here is the test input grid (in JSON format):\n",
    "\n",
    "{test_input}\n",
    "\n",
    "Generate the output grid that correctly applies the inferred transformation to this test input.\n",
    "\n",
    "Your response should be formatted strictly as a JSON code block in the following form:\n",
    "\n",
    "```json\n",
    "{{\n",
    "    \"output\": [[...]]\n",
    "}}\n",
    "```\"\"\"\n",
    "\n",
    "# Output template (for model-predicted grid)\n",
    "ARC_OUTPUT_TEMPLATE = \"\"\"```json\n",
    "{{\n",
    "    \"output\": {output_grid}\n",
    "}}\n",
    "```\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb559d1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_examples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Suppose you have these from your dataset\u001b[39;00m\n\u001b[32m      4\u001b[39m examples_json = \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join([\n\u001b[32m      5\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExample \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + json.dumps(ex, indent=\u001b[32m4\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, ex \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mtrain_examples\u001b[49m)\n\u001b[32m      7\u001b[39m ])\n\u001b[32m      9\u001b[39m test_input_json = json.dumps(test_input, indent=\u001b[32m4\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Build the final prompt\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'train_examples' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Suppose you have these from your dataset\n",
    "examples_json = \"\\n\".join([\n",
    "    f\"Example {i+1}:\\n\" + json.dumps(ex, indent=4)\n",
    "    for i, ex in enumerate(train_examples)\n",
    "])\n",
    "\n",
    "test_input_json = json.dumps(test_input, indent=4)\n",
    "\n",
    "# Build the final prompt\n",
    "final_prompt = ARC_PROMPT_TEMPLATE.format(\n",
    "    examples=examples_json,\n",
    "    test_input=test_input_json\n",
    ")\n",
    "\n",
    "# When generating the model output\n",
    "output_grid = json.dumps(predicted_output, indent=4)\n",
    "final_output = ARC_OUTPUT_TEMPLATE.format(output_grid=output_grid)\n",
    "\n",
    "print(final_prompt)\n",
    "print(final_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d23bde11",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"../data/arc-agi-2025/prompts\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0af5f8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_prompts_path = os.path.join(\"../data/arc-agi-2025/prompts/arc-agi_training_prompts.json\")\n",
    "validation_prompts_path = os.path.join(\"../data/arc-agi-2025/prompts/arc-agi_validation_prompts.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "926cb656",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(training_challenges_path, 'r') as f:\n",
    "    training_challenges = json.load(f)\n",
    "\n",
    "with open(training_solutions_path, 'r') as f:\n",
    "    training_solutions = json.load(f)\n",
    "\n",
    "with open(validation_challenges_path, 'r') as f:\n",
    "    validation_challenges = json.load(f)\n",
    "\n",
    "with open(validation_solutions_path, 'r') as f:\n",
    "    validation_solutions = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d38ccc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompts(challenges, solutions, save_path):\n",
    "    prompts = []\n",
    "    for challenge_id, challenge in challenges.items():\n",
    "        solution = solutions.get(challenge_id, [])\n",
    "        if solution == []:\n",
    "            continue\n",
    "        train_examples = challenge.get('train', [])\n",
    "        test_inputs = challenge.get('test', [{}])\n",
    "        if train_examples == []:\n",
    "            continue\n",
    "        examples_json = \"\\n\".join([\n",
    "            f\"Example {i+1}:\\n\" + json.dumps(ex, indent=4)\n",
    "            for i, ex in enumerate(train_examples)\n",
    "        ])\n",
    "        for i in range(len(test_inputs)):\n",
    "            test_input = test_inputs[i]\n",
    "            test_output = solution[i]\n",
    "            if not test_input.get('input'):\n",
    "                continue\n",
    "            test_input_json = json.dumps(test_input.get('input', []), indent=4)\n",
    "            final_prompt = ARC_PROMPT_TEMPLATE.format(\n",
    "                examples=examples_json,\n",
    "                test_input=test_input_json\n",
    "            )\n",
    "            challenge_id_new = challenge_id\n",
    "            if(len(test_inputs)>1):\n",
    "                challenge_id_new = f\"{challenge_id}_test{i}\"\n",
    "            output = ARC_OUTPUT_TEMPLATE.format(output_grid=json.dumps(test_output, indent=4))\n",
    "            prompts.append({\n",
    "                \"challenge_id\": challenge_id_new,\n",
    "                \"prompt\": final_prompt,\n",
    "                \"output\": output\n",
    "            })\n",
    "    \n",
    "    with open(save_path, 'w') as f:\n",
    "        json.dump(prompts, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2daec2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_prompts(training_challenges, training_solutions, training_prompts_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22088573",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_prompts(validation_challenges, validation_solutions, validation_prompts_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1212f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(training_prompts_path, 'r') as f:\n",
    "    training_prompts = json.load(f)\n",
    "\n",
    "with open(validation_prompts_path, 'r') as f:\n",
    "    validation_prompts = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "911d0322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1076"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7da2dfa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b381566c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_challenges_path = os.path.join(\"../data/arc-agi-2025/raw/arc-agi_evaluation_challenges.json\")\n",
    "evaluation_solutions_path = os.path.join(\"../data/arc-agi-2025/raw/arc-agi_evaluation_solutions.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fff33cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ARC-AGI Prompt and Output Templates\n",
    "# ============================================\n",
    "\n",
    "# Prompt template\n",
    "ARC_PROMPT_TEMPLATE = \"\"\"You are given example input-output grid pairs from the ARC (Abstraction and Reasoning Corpus) task.\n",
    "Each grid is represented as a 2D array of integers ranging from 0 to 9. Each integer corresponds to a specific color:\n",
    "\n",
    "0 - Black  \n",
    "1 - Blue  \n",
    "2 - Red  \n",
    "3 - Green  \n",
    "4 - Yellow  \n",
    "5 - Gray  \n",
    "6 - Magenta  \n",
    "7 - Orange  \n",
    "8 - Light Blue  \n",
    "9 - Dark Red  \n",
    "\n",
    "Each cell in the grid represents one colored square. The transformation from input to output follows a specific visual or logical pattern.\n",
    "\n",
    "Your task:\n",
    "1. Study the given example input-output pairs carefully.\n",
    "2. Some examples may be incorrect or noisy — identify the pattern that the *majority* of examples follow.\n",
    "3. Infer the correct transformation rule that maps the input grid to the output grid.\n",
    "4. Apply this inferred transformation to the provided test input grid to produce the correct output grid.\n",
    "\n",
    "Below are the example pairs (in JSON format):\n",
    "\n",
    "{examples}\n",
    "\n",
    "Now, here is the test input grid (in JSON format):\n",
    "\n",
    "{test_input}\n",
    "\n",
    "Generate the output grid that correctly applies the inferred transformation to this test input.\n",
    "\n",
    "Your response should be formatted strictly as a JSON code block in the following form:\n",
    "\n",
    "```json\n",
    "{{\n",
    "    \"output\": [[...]]\n",
    "}}\n",
    "```\"\"\"\n",
    "\n",
    "# Output template (for model-predicted grid)\n",
    "ARC_OUTPUT_TEMPLATE = \"\"\"```json\n",
    "{{\n",
    "    \"output\": {output_grid}\n",
    "}}\n",
    "```\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c8bf0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(evaluation_challenges_path, 'r') as f:\n",
    "    evaluation_challenges = json.load(f)\n",
    "\n",
    "\n",
    "with open(evaluation_solutions_path, 'r') as f:\n",
    "    evaluation_solutions = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6914b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompts_evaluation(challenges, solutions, save_path):\n",
    "    prompts = []\n",
    "    for challenge_id, challenge in challenges.items():\n",
    "        solution = solutions.get(challenge_id, [])\n",
    "        if solution == []:\n",
    "            continue\n",
    "        train_examples = challenge.get('train', [])\n",
    "        test_inputs = challenge.get('test', [{}])\n",
    "        if train_examples == []:\n",
    "            continue\n",
    "        examples_json = \"\\n\".join([\n",
    "            f\"Example {i+1}:\\n\" + json.dumps(ex, indent=4)\n",
    "            for i, ex in enumerate(train_examples)\n",
    "        ])\n",
    "        for i in range(len(test_inputs)):\n",
    "            test_input = test_inputs[i]\n",
    "            test_output = solution[i]\n",
    "            if not test_input.get('input'):\n",
    "                continue\n",
    "            test_input_json = json.dumps(test_input.get('input', []), indent=4)\n",
    "            final_prompt = ARC_PROMPT_TEMPLATE.format(\n",
    "                examples=examples_json,\n",
    "                test_input=test_input_json\n",
    "            )\n",
    "            challenge_id_new = challenge_id\n",
    "            if(len(test_inputs)>1):\n",
    "                challenge_id_new = f\"{challenge_id}_test{i}\"\n",
    "            output = ARC_OUTPUT_TEMPLATE.format(output_grid=json.dumps(test_output, indent=4))\n",
    "            prompts.append({\n",
    "                \"challenge_id\": challenge_id_new,\n",
    "                \"prompt\": final_prompt,\n",
    "                \"output\": output\n",
    "            })\n",
    "    \n",
    "    with open(save_path, 'w') as f:\n",
    "        json.dump(prompts, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7985d59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_prompts_path = os.path.join(\"../data/arc-agi-2025/prompts/arc-agi_evaluation_prompts.json\")\n",
    "generate_prompts_evaluation(evaluation_challenges, evaluation_solutions, evaluation_prompts_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21dac055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "# Wrap every solution value in a dict under \"output\"\n",
    "fixed = {k: {\"output\": v} for k, v in evaluation_solutions.items()}\n",
    "\n",
    "with open(\"../data/arc-agi-2025/evaluation/arc-agi_evaluation_solutions_fixed.json\", \"w\") as f:\n",
    "    json.dump(fixed, f, indent=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
