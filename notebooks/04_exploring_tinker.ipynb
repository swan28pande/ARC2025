{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98692e90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tinker\n",
    "from tinker import types\n",
    "import dotenv\n",
    "import os\n",
    "import json\n",
    "# Clean async training loop (improved/standalone). Paste/run after setup cells.\n",
    "import asyncio\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from typing import List, Iterator\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2e13340",
   "metadata": {},
   "outputs": [],
   "source": [
    "TINKER_API_KEY = os.getenv(\"TINKER_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9801ae95",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_client = tinker.ServiceClient(api_key=TINKER_API_KEY)\n",
    "base_model = \"Qwen/Qwen3-30B-A3B-Instruct-2507\"\n",
    "training_client = service_client.create_lora_training_client(\n",
    "    base_model=base_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "900da3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhushanshah/Documents/ARC-AGI-2/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Get the tokenizer from the training client\n",
    "tokenizer = training_client.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bbd8421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [3838, 374, 264, 1803, 30]\n"
     ]
    }
   ],
   "source": [
    "token_ids = tokenizer.encode(\"What is a car?\", add_special_tokens=True)\n",
    "print(\"Token IDs:\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23657820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [4340, 525, 498]\n"
     ]
    }
   ],
   "source": [
    "token_ids = tokenizer.encode(\"How are you\", add_special_tokens=True)\n",
    "print(\"Token IDs:\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1365b37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|im_end|>',\n",
       " '<|endoftext|>',\n",
       " '<|im_start|>',\n",
       " '<|object_ref_start|>',\n",
       " '<|object_ref_end|>',\n",
       " '<|box_start|>',\n",
       " '<|box_end|>',\n",
       " '<|quad_start|>',\n",
       " '<|quad_end|>',\n",
       " '<|vision_start|>',\n",
       " '<|vision_end|>',\n",
       " '<|vision_pad|>',\n",
       " '<|image_pad|>',\n",
       " '<|video_pad|>']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e4eb2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create a sampling client. We need to transfer weights\n",
    "sampling_client = training_client.save_weights_and_get_sampling_client(name='testing-24oct')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eeda73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now, we can sample from the model.\n",
    "prompt=types.ModelInput.from_ints(tokenizer.encode(\"What is a car?\"))\n",
    "params = types.SamplingParams(max_tokens=20, temperature=0.0, stop=[\"\\n\"]) # Greedy sampling\n",
    "future = sampling_client.sample(prompt=prompt, sampling_params=params, num_samples=8)\n",
    "result = future.result()\n",
    "print(\"Responses:\")\n",
    "for i, seq in enumerate(result.sequences):\n",
    "    print(f\"{i}: {repr(tokenizer.decode(seq.tokens))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996344cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Get the tokenizer from the training client\n",
    "tokenizer = training_client.get_tokenizer()\n",
    "\n",
    "training_prompts_path = os.path.join(\"../data/arc-agi-2025/prompts/arc-agi_training_prompts.json\")\n",
    "validation_prompts_path = os.path.join(\"../data/arc-agi-2025/prompts/arc-agi_validation_prompts.json\")\n",
    "\n",
    "with open(training_prompts_path, 'r') as f:\n",
    "    training_prompts = json.load(f)\n",
    "\n",
    "with open(validation_prompts_path, 'r') as f:\n",
    "    validation_prompts = json.load(f)\n",
    "\n",
    "class Config:\n",
    "    epochs = 5\n",
    "    batch_size = 64\n",
    "    learning_rate = 1e-4\n",
    "    max_sequence_length = 32500\n",
    "    lora_rank = 32\n",
    "\n",
    "    def __init__(self, epochs=5, batch_size=64, learning_rate=1e-4, max_sequence_length=32500, lora_rank=32):\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.lora_rank = lora_rank\n",
    "\n",
    "config = Config(learning_rate=5e-4, batch_size=64, epochs=5, max_sequence_length=32500, lora_rank=32)\n",
    "\n",
    "new_training_prompts = []\n",
    "new_validation_prompts = []\n",
    "for prompt in training_prompts:\n",
    "    input_ids = tokenizer.encode(prompt['prompt'] + prompt['output'])\n",
    "    if len(input_ids) <= config.max_sequence_length:\n",
    "        new_training_prompts.append(prompt)\n",
    "\n",
    "for prompt in validation_prompts:\n",
    "    input_ids = tokenizer.encode(prompt['prompt'] + prompt['output'])\n",
    "    if len(input_ids) <= config.max_sequence_length:\n",
    "        new_validation_prompts.append(prompt)\n",
    "\n",
    "def process_example(example: dict, tokenizer) -> types.Datum:\n",
    "    # Format the input with Input/Output template\n",
    "    # For most real use cases, you'll want to use a renderer / chat template,\n",
    "    # (see later docs) but here, we'll keep it simple.\n",
    "    prompt = example['prompt']\n",
    "    \n",
    "    prompt_tokens = tokenizer.encode(prompt, add_special_tokens=True)\n",
    "    prompt_weights = [0] * len(prompt_tokens)\n",
    "    # Add a space before the output string, and finish with double newline\n",
    "    completion_tokens = tokenizer.encode(example['output'], add_special_tokens=False)\n",
    "    completion_weights = [1] * len(completion_tokens)\n",
    " \n",
    "    tokens = prompt_tokens + completion_tokens\n",
    "    weights = prompt_weights + completion_weights\n",
    " \n",
    "    input_tokens = tokens[:-1]\n",
    "    target_tokens = tokens[1:] # We're predicting the next token, so targets need to be shifted.\n",
    "    weights = weights[1:]\n",
    " \n",
    "    # A datum is a single training example for the loss function.\n",
    "    # It has model_input, which is the input sequence that'll be passed into the LLM,\n",
    "    # loss_fn_inputs, which is a dictionary of extra inputs used by the loss function.\n",
    "    return types.Datum(\n",
    "        model_input=types.ModelInput.from_ints(tokens=input_tokens),\n",
    "        loss_fn_inputs=dict(weights=weights, target_tokens=target_tokens)\n",
    "    )\n",
    "\n",
    "processed_training_examples = [process_example(ex, tokenizer) for ex in new_training_prompts]\n",
    "processed_validation_examples = [process_example(ex, tokenizer) for ex in new_validation_prompts]\n",
    "\n",
    "\n",
    "\n",
    "# Preconditions (these are created in earlier cells):\n",
    "assert 'training_client' in globals(), 'training_client not found. Run the earlier cell that creates it.'\n",
    "assert 'processed_training_examples' in globals(), 'processed_training_examples not found. Run preprocessing.'\n",
    "assert 'config' in globals(), 'config not found. Define or import Config.'\n",
    "\n",
    "def batch_generator(examples: List[types.Datum], batch_size: int, shuffle: bool = True) -> Iterator[List[types.Datum]]:\n",
    "    idxs = list(range(len(examples)))\n",
    "    if shuffle:\n",
    "        random.shuffle(idxs)\n",
    "    for i in range(0, len(idxs), batch_size):\n",
    "        yield [examples[j] for j in idxs[i:i+batch_size]]\n",
    "\n",
    "def collate_batch(batch: List[types.Datum]) -> List[types.Datum]:\n",
    "    # tinker typically accepts a list of Datum; pad here if your client requires tensors\n",
    "    return batch\n",
    "\n",
    "def compute_mean_nll(logprobs_list, weights_list):\n",
    "    total = 0.0\n",
    "    total_w = 0.0\n",
    "    for lp_seq, w_seq in zip(logprobs_list, weights_list):\n",
    "        # Handle TensorData or similar wrappers\n",
    "        if hasattr(lp_seq, 'tolist'):\n",
    "            lp_seq = lp_seq.tolist()\n",
    "        elif hasattr(lp_seq, 'to_numpy'):\n",
    "            lp_seq = lp_seq.to_numpy()\n",
    "\n",
    "        if hasattr(w_seq, 'tolist'):\n",
    "            w_seq = w_seq.tolist()\n",
    "        elif hasattr(w_seq, 'to_numpy'):\n",
    "            w_seq = w_seq.to_numpy()\n",
    "\n",
    "        lp = np.array(lp_seq, dtype=float)\n",
    "        w = np.array(w_seq, dtype=float)\n",
    "\n",
    "        # ensure equal length\n",
    "        if lp.shape != w.shape:\n",
    "            min_len = min(lp.shape[0], w.shape[0])\n",
    "            lp = lp[:min_len]\n",
    "            w = w[:min_len]\n",
    "\n",
    "        total += (-lp * w).sum()\n",
    "        total_w += w.sum()\n",
    "\n",
    "    return float(total / total_w) if total_w > 0 else None\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = getattr(config, 'epochs', 5)\n",
    "batch_size = getattr(config, 'batch_size', 64)\n",
    "base_lr = getattr(config, 'learning_rate', 5e-4)\n",
    "checkpoint_dir = './tinker_checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "def now():\n",
    "    return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n",
    "\n",
    "def save_checkpoint(step_num: int, metadata: dict = None):\n",
    "    checkpoint_meta = {'step': step_num, 'timestamp': now()}\n",
    "    if metadata:\n",
    "        checkpoint_meta.update(metadata)\n",
    "    path = os.path.join(checkpoint_dir, f'checkpoint_step_{step_num}.json')\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(checkpoint_meta, f)\n",
    "    print(f'[{now()}] Saved local checkpoint metadata -> {path}')\n",
    "    try:\n",
    "        if hasattr(training_client, 'save_checkpoint'):\n",
    "            training_client.save_checkpoint(name=f'checkpoint_step_{step_num}')\n",
    "    except Exception as e:\n",
    "        print(f'[{now()}] Warning: remote checkpoint save failed: {e}')\n",
    "\n",
    "# Async wrappers that prefer the async client API and fall back to sync calls in executor\n",
    "async def call_forward_backward(batch):\n",
    "    if hasattr(training_client, 'forward_backward_async'):\n",
    "        maybe_coro = training_client.forward_backward_async(batch, loss_fn='cross_entropy')\n",
    "        res = await maybe_coro if asyncio.iscoroutine(maybe_coro) else maybe_coro\n",
    "        # handle future-like result objects returned by Tinker async APIs\n",
    "        if hasattr(res, 'result_async'):\n",
    "            return await res.result_async()\n",
    "        if hasattr(res, 'result'):\n",
    "            return res.result()\n",
    "        return res\n",
    "    else:\n",
    "        loop = asyncio.get_running_loop()\n",
    "        return await loop.run_in_executor(None, training_client.forward_backward, batch)\n",
    "\n",
    "async def call_optim_step(adam_params):\n",
    "    if hasattr(training_client, 'optim_step_async'):\n",
    "        maybe_coro = training_client.optim_step_async(adam_params)\n",
    "        res = await maybe_coro if asyncio.iscoroutine(maybe_coro) else maybe_coro\n",
    "        if hasattr(res, 'result_async'):\n",
    "            return await res.result_async()\n",
    "        if hasattr(res, 'result'):\n",
    "            return res.result()\n",
    "        return res\n",
    "    else:\n",
    "        loop = asyncio.get_running_loop()\n",
    "        return await loop.run_in_executor(None, training_client.optim_step, adam_params)\n",
    "\n",
    "async def train_async():\n",
    "    global_step = 0\n",
    "    num_examples = len(processed_training_examples)\n",
    "    steps_per_epoch = max(1, math.ceil(num_examples / batch_size))\n",
    "    print(f'[{now()}] Starting async training: epochs={epochs}, batch_size={batch_size}, base_lr={base_lr}')\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start = time.time()\n",
    "        epoch_loss_accum = 0.0\n",
    "        epoch_items = 0\n",
    "        for batch in batch_generator(processed_training_examples, batch_size, shuffle=True):\n",
    "            global_step += 1\n",
    "            batch = collate_batch(batch)\n",
    "\n",
    "            # linear LR schedule\n",
    "            step = global_step - 1\n",
    "            lr_mult = max(0.0, 1.0 - step / (steps_per_epoch * epochs))\n",
    "            current_lr = base_lr * lr_mult\n",
    "            adam_params = tinker.AdamParams(learning_rate=current_lr, beta1=0.9, beta2=0.95, eps=1e-8)\n",
    "\n",
    "            # forward/backward (async-aware)\n",
    "            try:\n",
    "                fwd_res = await call_forward_backward(batch)\n",
    "            except Exception as e:\n",
    "                print(f'[{now()}] forward_backward failed at step {global_step}: {e}. Retrying once...')\n",
    "                try:\n",
    "                    await asyncio.sleep(1.0)\n",
    "                    fwd_res = await call_forward_backward(batch)\n",
    "                except Exception as e2:\n",
    "                    print(f'[{now()}] Retry failed: {e2}. Skipping this batch.')\n",
    "                    continue\n",
    "\n",
    "            # optimizer step\n",
    "            try:\n",
    "                _ = await call_optim_step(adam_params)\n",
    "            except Exception as e:\n",
    "                print(f'[{now()}] optim_step failed at step {global_step}: {e}')\n",
    "\n",
    "            # extract per-sequence logprobs following cookbook pattern\n",
    "            train_logprobs = []\n",
    "            try:\n",
    "                lf_outputs = getattr(fwd_res, 'loss_fn_outputs', None)\n",
    "                if isinstance(lf_outputs, list):\n",
    "                    for entry in lf_outputs:\n",
    "                        if isinstance(entry, dict) and 'logprobs' in entry:\n",
    "                            train_logprobs.append(entry['logprobs'])\n",
    "                elif isinstance(lf_outputs, dict) and 'logprobs' in lf_outputs:\n",
    "                    train_logprobs = [lf_outputs['logprobs']]\n",
    "                else:\n",
    "                    # debug print once if unexpected structure\n",
    "                    print(f'[{now()}] Unexpected loss_fn_outputs structure (truncated): {str(lf_outputs)[:400]}')\n",
    "            except Exception as e:\n",
    "                print(f'[{now()}] Could not extract loss_fn_outputs: {e}')\n",
    "\n",
    "            train_weights = [d.loss_fn_inputs['weights'] for d in batch]\n",
    "            train_nll = compute_mean_nll(train_logprobs, train_weights) if train_logprobs else None\n",
    "\n",
    "            if train_nll is not None:\n",
    "                epoch_loss_accum += train_nll * len(batch)\n",
    "                epoch_items += len(batch)\n",
    "\n",
    "            if global_step % 10 == 0:\n",
    "                avg_loss_so_far = (epoch_loss_accum / epoch_items) if epoch_items > 0 else None\n",
    "                print(f'[{now()}] Epoch {epoch} step {global_step}  avg_loss_so_far={avg_loss_so_far} lr={current_lr}')\n",
    "\n",
    "            if global_step % 200 == 0:\n",
    "                save_checkpoint(global_step, metadata={'epoch': epoch, 'avg_loss_so_far': (epoch_loss_accum / epoch_items) if epoch_items else None})\n",
    "\n",
    "        # end epoch\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        epoch_avg_loss = (epoch_loss_accum / epoch_items) if epoch_items > 0 else None\n",
    "        print(f'[{now()}] Finished epoch {epoch}/{epochs} time={epoch_time:.1f}s avg_loss={epoch_avg_loss}')\n",
    "        save_checkpoint(global_step, metadata={'epoch': epoch})\n",
    "\n",
    "    print(f'[{now()}] Async training finished. Total steps: {global_step}')\n",
    "\n",
    "# Run training. In Jupyter, top-level await works; otherwise use asyncio.run(train_async()).\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(train_async())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
